{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash\n",
    "from flash.core.data.utils import download_data\n",
    "from flash.image import ObjectDetectionData, ObjectDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import io, base64\n",
    "from PIL import Image\n",
    "\n",
    "def convert_base64_img(base64_str):\n",
    "    img = Image.open(io.BytesIO(base64.decodebytes(bytes(base64_str, \"utf-8\"))))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824cc459",
   "metadata": {},
   "outputs": [],
   "source": [
    "flash.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f313d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utilities for skin AI project\"\"\"\n",
    "import os\n",
    "import os.path as op\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "CATEGORIES = [\n",
    "       {\"supercategory\": \"monkey\", \"id\": 1, \"name\": \"monkey\"},\n",
    "\n",
    "]\n",
    "\n",
    "def extract_bboxes(mask):\n",
    "    \"\"\"Compute bounding boxes from masks.\n",
    "    mask: [height, width, num_instances]. Mask pixels are either 1 or 0.\n",
    "    Returns: bbox array [num_instances, (y1, x1, y2, x2)].\n",
    "    \"\"\"\n",
    "    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n",
    "    for i in range(mask.shape[-1]):\n",
    "        m = mask[:, :, i]\n",
    "        # Bounding box.\n",
    "        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n",
    "        vertical_indicies = np.where(np.any(m, axis=1))[0]\n",
    "        if horizontal_indicies.shape[0]:\n",
    "            x1, x2 = horizontal_indicies[[0, -1]]\n",
    "            y1, y2 = vertical_indicies[[0, -1]]\n",
    "            # x2 and y2 should not be part of the box. Increment by 1.\n",
    "            x2 += 1\n",
    "            y2 += 1\n",
    "        else:\n",
    "            # No mask for this instance. Might happen due to\n",
    "            # resizing or cropping. Set bbox to zeros\n",
    "            x1, x2, y1, y2 = 0, 0, 0, 0\n",
    "        boxes[i] = np.array([y1, x1, y2, x2])\n",
    "    return boxes.astype(np.int32)\n",
    "\n",
    "def read_annotation_file(path):\n",
    "    \"\"\"Read annotation file\"\"\"\n",
    "    return json.load(open(path, \"r\"))\n",
    "\n",
    "\n",
    "def read_annotation_shapes(path: str):\n",
    "    \"\"\"Read annotation shapes from a given JSON path\"\"\"\n",
    "    return json.load(open(path, \"r\"))[\"shapes\"]\n",
    "\n",
    "\n",
    "def convert_points_to_polygon(points):\n",
    "    \"\"\"Convert points to polygon as list of tuples\"\"\"\n",
    "    return [tuple(l) for l in points]\n",
    "\n",
    "\n",
    "def split_dataset(df: pd.DataFrame):\n",
    "    \"\"\"Split a given dataframe into training, validation, and test set\"\"\"\n",
    "    df_train, df_val = train_test_split(df, test_size=0.15, random_state=42)\n",
    "    df_val, df_test = train_test_split(df_val, test_size=0.5, random_state=42)\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def create_df_from_dir(path_dir: str, output_size: bool = False):\n",
    "    \"\"\"\n",
    "    Create training dataframe from directory\n",
    "    \"\"\"\n",
    "    img_paths = glob(f\"{path_dir}/*.jpg\")\n",
    "    annotation_paths = glob(f\"{path_dir}/*.json\")\n",
    "    img_df = pd.DataFrame(img_paths, columns=[\"image_path\"])\n",
    "    annotation_df = pd.DataFrame(annotation_paths, columns=[\"annotation_path\"])\n",
    "    img_df[\"img_name\"] = img_df.image_path.map(lambda x: op.basename(x).replace(\".jpg\", \"\"))\n",
    "    annotation_df[\"img_name\"] = annotation_df.annotation_path.map(\n",
    "        lambda x: op.basename(x).replace(\".json\", \"\")\n",
    "    )\n",
    "    df = img_df.merge(annotation_df, on=\"img_name\")\n",
    "\n",
    "    if output_size:\n",
    "        print(f\"Number of image: {len(img_paths)}\")\n",
    "        print(f\"Number of annotation JSON: {len(annotation_paths)}\")\n",
    "        print(f\"Total number of : {len(df)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_df_from_dir_(img_dir: str, annoatation_dir: str, output_size: bool = False):\n",
    "    \"\"\"\n",
    "    Create training dataframe from directory\n",
    "    \"\"\"\n",
    "    img_paths = glob(f\"{img_dir}/*.jpg\")\n",
    "    annotation_paths = glob(f\"{annoatation_dir}/*.json\")\n",
    "    img_df = pd.DataFrame(img_paths, columns=[\"image_path\"])\n",
    "    annotation_df = pd.DataFrame(annotation_paths, columns=[\"annotation_path\"])\n",
    "    img_df[\"img_name\"] = img_df.image_path.map(lambda x: op.basename(x).replace(\".jpg\", \"\"))\n",
    "    annotation_df[\"img_name\"] = annotation_df.annotation_path.map(\n",
    "        lambda x: op.basename(x).replace(\"_c.json\", \"\")\n",
    "    )\n",
    "    df = img_df.merge(annotation_df, on=\"img_name\")\n",
    "\n",
    "    if output_size:\n",
    "        print(f\"Number of image: {len(img_paths)}\")\n",
    "        print(f\"Number of annotation JSON: {len(annotation_paths)}\")\n",
    "        print(f\"Total number of : {len(df)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_dataset(df: pd.DataFrame, output_path: str):\n",
    "    \"\"\"\n",
    "    Copy dataset to output path from a given dataframe.\n",
    "    Dataframe should contain `image_path` and `annotation_path` in the columns.\n",
    "    \"\"\"\n",
    "    if not op.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        print(f\"Create {output_path} since it doesn't exist before\")\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        image_name = op.basename(row[\"image_path\"])\n",
    "        annotation_name = op.basename(row[\"annotation_path\"])\n",
    "        dest_image_path = op.join(output_path, image_name)\n",
    "        dest_annotation_path = op.join(output_path, annotation_name)\n",
    "        if not op.exists(dest_image_path):\n",
    "            copyfile(row[\"image_path\"], dest_image_path)\n",
    "        if not op.exists(dest_annotation_path):\n",
    "            copyfile(row[\"annotation_path\"], dest_annotation_path)\n",
    "\n",
    "\n",
    "def polygon_to_mask(image, polygon):\n",
    "    \"\"\"\n",
    "    Convert polygon to mask from a given image\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    mask = Image.new(\"1\", (width, height), 0)\n",
    "    ImageDraw.Draw(mask).polygon(polygon, outline=1, fill=1)\n",
    "    mask = np.array(mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def plot_poly(\n",
    "    image_path: str,\n",
    "    poly: list,\n",
    "    is_resize: bool = True,\n",
    "    image_size: Optional[tuple] = (1000, 1000),\n",
    "    alpha: float = 0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot polygon on top of image.\n",
    "\n",
    "    image_path: str, path to image\n",
    "    alpha: float, blending ratio\n",
    "\n",
    "    Example\n",
    "    =======\n",
    "    >>> image = Image.open(row[\"image_path\"])\n",
    "    >>> poly = convert_points_to_polygon(read_annotation_file(row[\"annotation_path\"])[0][\"points\"])\n",
    "    >>> plot_poly(image, poly)\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image2 = image.copy()\n",
    "    draw = ImageDraw.Draw(image2)\n",
    "    draw.polygon(poly, fill=\"red\")\n",
    "    image_blend = Image.blend(image, image2, alpha)\n",
    "\n",
    "    if not is_resize:\n",
    "        return image, image_blend\n",
    "\n",
    "    image_blend_resize = ImageOps.contain(image_blend, image_size)\n",
    "    image = ImageOps.contain(image, image_size)\n",
    "    return image, image_blend_resize\n",
    "\n",
    "\n",
    "def create_coco_data_dict(\n",
    "    path: str,\n",
    "    labels: list = [\"melasma\", \"hori nevus\", \"solar lentigines\"],\n",
    "    start=0,\n",
    "    categories=CATEGORIES,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create COCO dataset to be saved in JSON format from a given path.\n",
    "    Path should contain images and annotations.\n",
    "\n",
    "    path: str, path to image and annotation JSON files\n",
    "    labels: list, interested labels\n",
    "    start: int, default 0, starting index of the image index\n",
    "    categories: list, default CATEGORIES, COCO categories\n",
    "    \"\"\"\n",
    "    # map between class name and id\n",
    "    categories_dict = {d[\"name\"].lower(): d[\"id\"] for d in categories}\n",
    "\n",
    "    df = create_df_from_dir(path)\n",
    "    images, annotations = [], []\n",
    "    for i, r in tqdm(df.iterrows(), total=len(df)):\n",
    "        image_id = start + i\n",
    "        image_path = r.image_path\n",
    "        annotation_path = r.annotation_path\n",
    "        image_name = op.basename(image_path)\n",
    "\n",
    "        # Read annotation file\n",
    "        raw_annotation_info = read_annotation_file(annotation_path)\n",
    "        raw_annotations = raw_annotation_info[\"shapes\"]\n",
    "\n",
    "        # Calculate image size ratio from annotation file with input image\n",
    "        original_image_width = raw_annotation_info[\"imageWidth\"]\n",
    "        original_image_height = raw_annotation_info[\"imageHeight\"]\n",
    "\n",
    "        raw_annotations = read_annotation_file(annotation_path)[\"shapes\"]\n",
    "        for annotation in raw_annotations:\n",
    "            label = annotation[\"label\"].lower()\n",
    "            if label in labels:\n",
    "                category_id = categories_dict[label]\n",
    "                image = Image.open(image_path)\n",
    "                image_width, image_height = image.size\n",
    "\n",
    "                resize_raito = original_image_height / image_height\n",
    "\n",
    "                if len(annotation.get(\"points\")) is not None:\n",
    "                    # points = annotation[\"points\"]\n",
    "                    # Calculate new position points\n",
    "                    points = np.array(annotation[\"points\"])\n",
    "                    points = (points / resize_raito).round()\n",
    "\n",
    "                    polygon = convert_points_to_polygon(points)\n",
    "\n",
    "                    if len(polygon) <= 1:\n",
    "                        continue\n",
    "\n",
    "                    segmentation = np.hstack(polygon)\n",
    "                    mask = polygon_to_mask(image, polygon)\n",
    "                    masks = np.expand_dims(mask, -1)\n",
    "\n",
    "                    # Create bbox bbox array [num_instances, (y1, x1, y2, x2)].\n",
    "                    boxes = extract_bboxes(masks)\n",
    "                    bbox = boxes[0]\n",
    "                    width = bbox[3] - bbox[1]\n",
    "                    height = bbox[2] - bbox[0]\n",
    "                    coco_bbox = [int(bbox[1]), int(bbox[0]), int(width), int(height)]\n",
    "                    area = float(width * height)\n",
    "                    image_dict = {\n",
    "                        \"id\": image_id,\n",
    "                        \"width\": image_width,\n",
    "                        \"height\": image_height,\n",
    "                        \"file_name\": image_name,\n",
    "                        \"file_path\": image_path,\n",
    "                    }\n",
    "                    annotation_dict = {\n",
    "                        \"id\": image_id,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"label\": label,\n",
    "                        \"category_id\": category_id,\n",
    "                        \"segmentation\": [segmentation.tolist()],\n",
    "                        \"bbox\": coco_bbox,\n",
    "                        \"iscrowd\": False,\n",
    "                        \"area\": area,\n",
    "                        \"original_bbox\": bbox.tolist(),\n",
    "                        \"points\": points.tolist(),\n",
    "                    }\n",
    "                    images.append(image_dict)\n",
    "                    annotations.append(annotation_dict)\n",
    "    coco_data_dict = {\n",
    "        \"categories\": categories,\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "    }\n",
    "    return coco_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c09a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\n",
    "    {\"supercategory\": \"monkey\", \"id\": 1, \"name\": \"monkey\"},\n",
    "]\n",
    "\n",
    "def create_coco_data_dict(paths: list, labels: list = [\"monkey\"], start: int = 0, categories: list = CATEGORIES):\n",
    "    \"\"\"Loop to all paths to labelme JSON and create COCO dataset format in dictionary format\"\"\"\n",
    "    categories_dict = {d[\"name\"].lower(): d[\"id\"] for d in categories}\n",
    "    images, annotations = [], []\n",
    "    for i, path in tqdm(enumerate(paths)):\n",
    "        image_id = start + i\n",
    "        raw_annotations = read_annotation_file(path)\n",
    "        image = convert_base64_img(raw_annotations[\"imageData\"])\n",
    "        shapes = raw_annotations[\"shapes\"]\n",
    "        original_image_width = raw_annotations[\"imageWidth\"]\n",
    "        original_image_height = raw_annotations[\"imageHeight\"]\n",
    "        image_name = raw_annotations[\"imagePath\"]\n",
    "        image_path = os.path.join(\"train\", raw_annotations[\"imagePath\"])\n",
    "        \n",
    "        image_width, image_height = image.size\n",
    "        image_dict = {\n",
    "            \"id\": image_id,\n",
    "            \"width\": image_width,\n",
    "            \"height\": image_height,\n",
    "            \"file_name\": image_name,\n",
    "            \"file_path\": image_path,\n",
    "        }\n",
    "        images.append(image_dict)\n",
    "        for annotation in shapes:\n",
    "            label = annotation[\"label\"].lower()\n",
    "            if label in labels:\n",
    "                category_id = categories_dict[label]\n",
    "                image_width, image_height = image.size\n",
    "\n",
    "                resize_raito = original_image_height / image_height\n",
    "\n",
    "                if len(annotation.get(\"points\")) is not None:\n",
    "                    # points = annotation[\"points\"]\n",
    "                    # Calculate new position points\n",
    "                    points = np.array(annotation[\"points\"])\n",
    "                    points = (points / resize_raito).round()\n",
    "\n",
    "                    polygon = convert_points_to_polygon(points)\n",
    "\n",
    "                    if len(polygon) <= 1:\n",
    "                        continue\n",
    "\n",
    "                    segmentation = np.hstack(polygon)\n",
    "                    mask = polygon_to_mask(image, polygon)\n",
    "                    masks = np.expand_dims(mask, -1)\n",
    "\n",
    "                    # Create bbox bbox array [num_instances, (y1, x1, y2, x2)].\n",
    "                    boxes = extract_bboxes(masks)\n",
    "                    bbox = boxes[0]\n",
    "                    width = bbox[3] - bbox[1]\n",
    "                    height = bbox[2] - bbox[0]\n",
    "                    coco_bbox = [int(bbox[1]), int(bbox[0]), int(width), int(height)]\n",
    "                    area = float(width * height)\n",
    "                    annotation_dict = {\n",
    "                        \"id\": image_id,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"label\": label,\n",
    "                        \"category_id\": category_id,\n",
    "                        \"segmentation\": [segmentation.tolist()],\n",
    "                        \"bbox\": coco_bbox,\n",
    "                        \"iscrowd\": False,\n",
    "                        \"area\": area,\n",
    "                        \"original_bbox\": bbox.tolist(),\n",
    "                        \"points\": points.tolist(),\n",
    "                    }\n",
    "                    annotations.append(annotation_dict)\n",
    "    coco_data_dict = {\n",
    "        \"categories\": categories,\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "    }\n",
    "    return coco_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob(\"ano/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58433760",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_data_dict = create_coco_data_dict(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = json.load(open(\"instances_train2017.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f651a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(d, open(\"instances_train2017_indent.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4acd4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(coco_data_dict, open(\"coco_monkey_annotation.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "batch_size = 1\n",
    "max_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = ObjectDetectionData.from_coco(\n",
    "    train_folder=\"train/\",\n",
    "    train_ann_file=\"coco_monkey_annotation.json\",\n",
    "    batch_size=batch_size,\n",
    "    transform_kwargs={\"image_size\": image_size},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32df341",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash.image.detection.output import FiftyOneDetectionLabelsOutput\n",
    "\n",
    "# Set model output for fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dbd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = ObjectDetector(\n",
    "    head=\"retinanet\", \n",
    "    backbone=\"resnet18_fpn\", \n",
    "    num_classes=datamodule.num_classes, \n",
    "    image_size=image_size,\n",
    "    output=FiftyOneDetectionLabelsOutput(return_filepath=True),\n",
    "    learning_rate=0.0001,\n",
    "    pretrained=True\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d43708",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObjectDetector(\n",
    "    head=\"efficientdet\", \n",
    "    backbone=\"d0\", \n",
    "    num_classes=datamodule.num_classes, \n",
    "    image_size=image_size,\n",
    "    output=FiftyOneDetectionLabelsOutput(return_filepath=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = flash.Trainer(max_epochs=max_epochs, gpus=1)\n",
    "trainer.finetune(model, datamodule=datamodule, strategy=\"no_freeze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b542773",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d72810",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_files = glob(\"train/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = ObjectDetectionData.from_files(\n",
    "    predict_files=predict_files,\n",
    "    batch_size=1, \n",
    "    transform_kwargs={\"image_size\": image_size},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ddd097",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model, datamodule=predict_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b841fe",
   "metadata": {},
   "source": [
    "## Visualize with fiftyone\n",
    "\n",
    "- https://voxel51.com/docs/fiftyone/getting_started/install.html\n",
    "- install fiftyone (do not forget to restart jupyter notebook)\n",
    "```sh\n",
    "pip install fiftyone\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash.core.integrations.fiftyone import visualize\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "_predictions = list(chain.from_iterable(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248875de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "session = visualize(_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b1301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
